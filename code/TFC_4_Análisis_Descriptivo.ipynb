{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2pXqHOII47b"
      },
      "source": [
        "###Instalación Spark, creación de sesión y unidad personal de Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJzlyFtGJAuM"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chCRL25rJDi_"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gKVtmG2JFzW"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYNudI_zT0pg"
      },
      "outputs": [],
      "source": [
        "!wget https://downloads.apache.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLZF8yhVJGVA"
      },
      "outputs": [],
      "source": [
        "!wget -q http://apache.mirrors.pair.com/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTiREYDJJHz4"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkXfRMSCJXnY"
      },
      "outputs": [],
      "source": [
        "!tar xf spark-3.5.3-bin-hadoop3.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "d_7tTU18JZn0"
      },
      "outputs": [],
      "source": [
        "!pip install -q findspark\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcx470EQJb9h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.3-bin-hadoop3\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvSPLzPBJeoi"
      },
      "outputs": [],
      "source": [
        "findspark.find()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIwQOdgaJiWC"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import when\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Pyspark_SQL\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LZDWHPwbJkbM"
      },
      "outputs": [],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK5HNmMGJpIc"
      },
      "source": [
        "DESARROLLO, ejercicio 4:\n",
        "\n",
        "Cargar el conjunto de datos en un dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_NqHRJvJrua"
      },
      "outputs": [],
      "source": [
        "data_path = '/content/drive/MyDrive/TokioSchool/data_curso/'\n",
        "\n",
        "df = spark.read.options(header=True, inferSchema=True).csv(data_path + 'Air_Traffic_Passenger_Statistics.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUY-5Mqljdea"
      },
      "source": [
        "ANÁLISIS DESCRIPTIVO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nygiTU2NO7g1"
      },
      "source": [
        "- MEDIA DE CADA ELEMENTO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R6pH5aHp-pA"
      },
      "source": [
        "Media del número de pasajeros según la región, categoria de precio y el mes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xF2BHdFXL20j"
      },
      "outputs": [],
      "source": [
        "df_region = df.groupBy('GEO Region').agg (F.mean('Passenger Count').alias('mean_passenger_count')).show()\n",
        "\n",
        "df_price = df.groupBy('Price Category Code').agg (F.mean('Passenger Count').alias('mean_passenger_count')).show()\n",
        "\n",
        "df_month = df.groupBy('Month').agg (F.mean('Passenger Count').alias('mean_passenger_count')).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66L1bfaIPHTR"
      },
      "source": [
        "- DESVIACIÓN TÍPICA DE CADA ELEMENTO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v__6O0beqMEB"
      },
      "source": [
        "Desviación estandar del número de pasajeros según la región, categoria de precio y el mes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CXcuXxEzPK-q"
      },
      "outputs": [],
      "source": [
        "df_region = df.groupBy('GEO Region').agg (F.std('Passenger Count').alias('std_passenger_count')).show()\n",
        "\n",
        "df_price = df.groupBy('Price Category Code').agg (F.std('Passenger Count').alias('std_passenger_count')).show()\n",
        "\n",
        "df_month = df.groupBy('Month').agg (F.std('Passenger Count').alias('std_passenger_count')).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "impRPBEHgxIx"
      },
      "source": [
        "- Análisis de correlación\n",
        "- Matriz de correlación en el resultado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHNa4NQkOlTy"
      },
      "outputs": [],
      "source": [
        "# Transformo todas las variables para poder calcular la correlacion. Crear nuevas columnas\n",
        "operating_airline_indexer = StringIndexer(inputCol=\"Operating Airline\", outputCol=\"Operating Airline Index\")\n",
        "df = operating_airline_indexer.fit(df).transform(df)\n",
        "\n",
        "geo_summary_indexer = StringIndexer(inputCol=\"GEO Summary\", outputCol=\"GEO Summary Index\")\n",
        "df = geo_summary_indexer.fit(df).transform(df)\n",
        "\n",
        "geo_region_indexer = StringIndexer(inputCol=\"GEO Region\", outputCol=\"GEO Region Index\")\n",
        "df = geo_region_indexer.fit(df).transform(df)\n",
        "\n",
        "activity_type_code_indexer = StringIndexer(inputCol=\"Activity Type Code\", outputCol=\"Activity Type Code Index\")\n",
        "df = activity_type_code_indexer.fit(df).transform(df)\n",
        "\n",
        "price_category_code_indexer = StringIndexer(inputCol=\"Price Category Code\", outputCol=\"Price Category Code Index\")\n",
        "df = price_category_code_indexer.fit(df).transform(df)\n",
        "\n",
        "terminal_indexer = StringIndexer(inputCol=\"Terminal\", outputCol=\"Terminal Index\")\n",
        "df = terminal_indexer.fit(df).transform(df)\n",
        "\n",
        "boarding_area_indexer = StringIndexer(inputCol=\"Boarding Area\", outputCol=\"Boarding Area Index\")\n",
        "df = boarding_area_indexer.fit(df).transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVsPmgyqXy5D"
      },
      "outputs": [],
      "source": [
        "# Mapeo de meses a números\n",
        "df = df.withColumn ('month_numero',\n",
        "    when (df['Month'] == 'January', 1)\n",
        "    .when (df['Month'] == 'February', 2)\n",
        "    .when (df['Month'] == 'March', 3)\n",
        "    .when (df['Month'] == 'April', 4)\n",
        "    .when (df['Month'] == 'May', 5)\n",
        "    .when (df['Month'] == 'June', 6)\n",
        "    .when (df['Month'] == 'July', 7)\n",
        "    .when (df['Month'] == 'August', 8)\n",
        "    .when (df['Month'] == 'September', 9)\n",
        "    .when (df['Month'] == 'October', 10)\n",
        "    .when (df['Month'] == 'November', 11)\n",
        "    .when (df['Month'] == 'December', 12)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Lista de columnas numéricas\n",
        "numeric_cols = ['Operating Airline Index', 'GEO Summary Index', 'GEO Region Index', 'Activity Type Code Index', 'Price Category Code Index',\n",
        "                'Terminal Index', 'Boarding Area Index', 'Passenger Count', 'Year', 'month_numero']\n",
        "\n",
        "# Crear una lista para almacenar los resultados\n",
        "correlation_data = []\n",
        "\n",
        "# Calcular la correlación entre cada par de columnas\n",
        "for col1 in numeric_cols:\n",
        "    for col2 in numeric_cols:\n",
        "        corr_value = df.stat.corr(col1, col2)\n",
        "        correlation_data.append([col1, col2, corr_value])  # Añadir los resultados a la lista\n",
        "        print(f\"Correlación entre {col1} y {col2}: {corr_value}\")"
      ],
      "metadata": {
        "id": "WV-yi7lMXa5X",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO1WTh8DuQt4"
      },
      "source": [
        "-  Algoritmo: Árbol de decisión"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.regression import GBTRegressor\n",
        "\n",
        "# Crear una sesión de Spark\n",
        "spark = SparkSession.builder.appName(\"Air Traffic Passenger Prediction\").getOrCreate()\n",
        "\n",
        "# Cargar los datos\n",
        "df = spark.read.option(\"header\", True).csv(\"/content/drive/MyDrive/TokioSchool/data_curso/Air_Traffic_Passenger_Statistics.csv\")\n",
        "\n",
        "# Transformar columnas a indice\n",
        "operating_airline_indexer = StringIndexer(inputCol=\"Operating Airline\", outputCol=\"Operating Airline Index\")\n",
        "df = operating_airline_indexer.fit(df).transform(df)\n",
        "\n",
        "published_airline_indexer = StringIndexer(inputCol=\"Published Airline\", outputCol=\"Published Airline Index\")\n",
        "df = published_airline_indexer.fit(df).transform(df)\n",
        "\n",
        "GEO_summary_indexer = StringIndexer(inputCol=\"GEO Summary\", outputCol=\"GEO Summary Index\")\n",
        "df = GEO_summary_indexer.fit(df).transform(df)\n",
        "\n",
        "activity_type_code_indexer = StringIndexer(inputCol=\"Activity Type Code\", outputCol=\"Activity Type Code Index\")\n",
        "df = activity_type_code_indexer.fit(df).transform(df)\n",
        "\n",
        "price_category_code_indexer = StringIndexer(inputCol=\"Price Category Code\", outputCol=\"Price Category Code Index\")\n",
        "df = price_category_code_indexer.fit(df).transform(df)\n",
        "\n",
        "terminal_indexer = StringIndexer(inputCol=\"Terminal\", outputCol=\"Terminal Index\")\n",
        "df = terminal_indexer.fit(df).transform(df)\n",
        "\n",
        "boarding_area_indexer = StringIndexer(inputCol=\"Boarding Area\", outputCol=\"Boarding Area Index\")\n",
        "df = boarding_area_indexer.fit(df).transform(df)\n",
        "\n",
        "adjusted_activity_type_code_indexer = StringIndexer(inputCol=\"Adjusted Activity Type Code\", outputCol=\"Adjusted Activity Type Code Index\")\n",
        "df = adjusted_activity_type_code_indexer.fit(df).transform(df)\n",
        "\n",
        "month_indexer = StringIndexer(inputCol=\"Month\", outputCol=\"Month Index\")\n",
        "df = month_indexer.fit(df).transform(df)\n",
        "\n",
        "# Modificar tipos de datos\n",
        "df = df.withColumn('Activity Period', df['Activity Period'].cast('float'))\n",
        "df = df.withColumn('Operating Airline Index', df['Operating Airline Index'].cast('float'))\n",
        "df = df.withColumn('Published Airline Index', df['Published Airline Index'].cast('float'))\n",
        "df = df.withColumn('GEO Summary Index', df['GEO Summary Index'].cast('float'))\n",
        "df = df.withColumn('Activity Type Code Index', df['Activity Type Code Index'].cast('float'))\n",
        "df = df.withColumn('Price Category Code Index', df['Price Category Code Index'].cast('float'))\n",
        "df = df.withColumn('Terminal Index', df['Terminal Index'].cast('float'))\n",
        "df = df.withColumn('Boarding Area Index', df['Boarding Area Index'].cast('float'))\n",
        "df = df.withColumn('Passenger Count', df['Passenger Count'].cast('float'))\n",
        "df = df.withColumn('Adjusted Activity Type Code Index', df['Adjusted Activity Type Code Index'].cast('float'))\n",
        "df = df.withColumn('Year', df['Year'].cast('float'))\n",
        "df = df.withColumn('Month Index', df['Month Index'].cast('float'))\n",
        "\n",
        "# Crear la columna 'features' utilizando VectorAssembler\n",
        "va = VectorAssembler(inputCols=['Activity Period','Operating Airline Index', 'Published Airline Index', 'GEO Summary Index','Activity Type Code Index',\n",
        "                                'Price Category Code Index','Terminal Index','Boarding Area Index','Year', 'Adjusted Activity Type Code Index','Month Index'], outputCol='features')\n",
        "\n",
        "df = va.transform(df)\n",
        "\n",
        "# Dividir los datos en entrenamiento y prueba\n",
        "train, test = df.randomSplit([0.8, 0.2])\n",
        "\n",
        "# Definir el modelo de regresión basado en árboles de decisión\n",
        "rf = GBTRegressor(featuresCol='features', labelCol='Passenger Count', maxIter=100)\n",
        "\n",
        "# Entrenar el modelo\n",
        "rf_model = rf.fit(train)\n",
        "\n",
        "# Hacer predicciones\n",
        "predictions = rf_model.transform(test)\n",
        "\n",
        "# Evaluar el modelo\n",
        "evaluator = RegressionEvaluator(labelCol=\"Passenger Count\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "r2 = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"R2: {r2}\")\n",
        "\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Función para convertir el vector 'features' en una cadena de texto\n",
        "def vector_to_string(v):\n",
        "    return str(v)\n",
        "\n",
        "# Crear un UDF (User Defined Function) para aplicar la conversión\n",
        "vector_to_string_udf = udf(vector_to_string, StringType())\n",
        "\n",
        "# Aplicar el UDF para convertir la columna 'features' a texto\n",
        "df_with_string_features = predictions.withColumn('features_str', vector_to_string_udf(predictions['features']))\n",
        "\n",
        "# Seleccionar solo las columnas necesarias (incluyendo la nueva columna de características como cadena de texto)\n",
        "df_export = df_with_string_features.select('features_str', 'Passenger Count', 'prediction')\n",
        "\n",
        "# Exportar a CSV\n",
        "df_export.coalesce(1).write \\\n",
        "  .mode(\"overwrite\") \\\n",
        "  .format(\"csv\") \\\n",
        "  .option(\"header\", \"true\") \\\n",
        "  .save(\"/content/drive/MyDrive/TokioSchool/data_algoritmo/\")"
      ],
      "metadata": {
        "id": "Ukd68sj_6vYF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
